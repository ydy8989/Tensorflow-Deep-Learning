{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow_Tutorial.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/PeterCha90/Tensorflow-Deep-Learning/blob/master/Tensorflow_Tutorial.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "jnYUvmoR8AOk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Simple Softmax Network"
      ]
    },
    {
      "metadata": {
        "id": "2EHv_kRuy7kh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "4546dc52-adaa-4070-d1d9-9a7c65583aa8"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1714: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x1f11SQx0A1F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# placeholders \n",
        "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
        "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C6lJWtkO321A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Variable\n",
        "W = tf.Variable(tf.zeros([784, 10]))\n",
        "b = tf.Variable(tf.zeros([10]))\n",
        "\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VccVHINZ4Gou",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model & loss function\n",
        "y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
        "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4-Q_vS9Y5adO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# optimizer\n",
        "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kG6l7cM85p5G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# training\n",
        "for i in range(1000):\n",
        "    batch = mnist.train.next_batch(50)\n",
        "    train_step.run(feed_dict={x: batch[0], y_: batch[1]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pq1eLCKm6SbZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8547556a-1c7a-4f92-d945-cb5c9d0da862"
      },
      "cell_type": "code",
      "source": [
        "# evaluation\n",
        "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9192\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a4fRvhEx76TS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Convolution Neural Network\n"
      ]
    },
    {
      "metadata": {
        "id": "stfuxA8d7CSx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Weight initialization\n",
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "# Convolution & Pooling\n",
        "def conv2d(x, W):\n",
        "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
        "                          strides = [1, 2, 2, 1], padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7_ByxbFd9r-X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The First Convolution Layer"
      ]
    },
    {
      "metadata": {
        "id": "04TJzqTx75Ta",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 5x5=kernel size, 1=num_of_input_channel, 32=num_of_output_channel\n",
        "W_conv1 = weight_variable([5, 5, 1, 32])\n",
        "b_conv1 = bias_variable([32])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I6n__n6D9WLh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# reshape x into 4D tensor\n",
        "# 28x28 size, 1 color chnnel\n",
        "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
        "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
        "h_pool1 = max_pool_2x2(h_conv1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OkULmBBa-0EY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The Second Convolution Layer"
      ]
    },
    {
      "metadata": {
        "id": "ZRjRdqAh95mD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "W_conv2 = weight_variable([5, 5, 32, 64])\n",
        "b_conv2 = bias_variable([64])\n",
        "\n",
        "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
        "h_pool2 = max_pool_2x2(h_conv2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Ol_BrbrBzMV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Fully Connected Layer"
      ]
    },
    {
      "metadata": {
        "id": "xB-02ImG_jgs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
        "b_fc1 = bias_variable([1024])\n",
        "\n",
        "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
        "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i3E7ZeLWCW5O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Dropout"
      ]
    },
    {
      "metadata": {
        "id": "1DweFjOtCREv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "keep_prob = tf.placeholder(tf.float32)\n",
        "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MoFP-wvLFE8r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Last Softmax layer"
      ]
    },
    {
      "metadata": {
        "id": "V9A2l1aFFBDc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "W_fc2 = weight_variable([1024, 10])\n",
        "b_fc2 = bias_variable([10])\n",
        "\n",
        "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GcHBXyZOJvts",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Model training & Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "4KYZam7CFcUr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "b5de1bff-c1b1-4f73-cfa9-98340ebd6dc3"
      },
      "cell_type": "code",
      "source": [
        "# loss function\n",
        "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n",
        "\n",
        "# Optimizer\n",
        "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
        "\n",
        "# accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# training\n",
        "for i in range(20000):\n",
        "    batch = mnist.train.next_batch(50)\n",
        "    if i%1000 == 0:\n",
        "        train_accuracy = accuracy.eval(feed_dict={\n",
        "            x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
        "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
        "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
        "    \n",
        "print(\"test accuracy %G\"%accuracy.eval(feed_dict={\n",
        "    x: mnist.test.images, y_:mnist.test.labels, keep_prob: 1.0}))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, training accuracy 0.08\n",
            "step 1000, training accuracy 0.94\n",
            "step 2000, training accuracy 1\n",
            "step 3000, training accuracy 1\n",
            "step 4000, training accuracy 0.98\n",
            "step 5000, training accuracy 1\n",
            "step 6000, training accuracy 0.98\n",
            "step 7000, training accuracy 0.96\n",
            "step 8000, training accuracy 1\n",
            "step 9000, training accuracy 1\n",
            "step 10000, training accuracy 1\n",
            "step 11000, training accuracy 0.96\n",
            "step 12000, training accuracy 1\n",
            "step 13000, training accuracy 1\n",
            "step 14000, training accuracy 1\n",
            "step 15000, training accuracy 0.98\n",
            "step 16000, training accuracy 0.98\n",
            "step 17000, training accuracy 1\n",
            "step 18000, training accuracy 1\n",
            "step 19000, training accuracy 1\n",
            "test accuracy 0.9934\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gjzAwxC5KKsv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}