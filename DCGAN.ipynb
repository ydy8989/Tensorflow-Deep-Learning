{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN\n",
    "\n",
    "* [DCGAN](https://arxiv.org/pdf/1511.06434.pdf) with Fashion MNIST data.\n",
    "* First of all, you should import [GAN.py](https://github.com/tensorpack/tensorpack/blob/master/examples/GAN/GAN.py) at the same path.\n",
    "* [Reference](https://github.com/tensorpack/tensorpack/blob/master/examples/GAN/DCGAN.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  \n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "from GAN import *\n",
    "from PIL import Image\n",
    "from tensorpack import *\n",
    "from tensorpack.dataflow import *\n",
    "from tensorpack.callbacks import *\n",
    "from tensorpack.tfutils import summary\n",
    "from tensorpack.utils.viz import stack_patches\n",
    "from tensorpack.tfutils.scope_utils import auto_reuse_variable_scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataflow(batch_size, is_train='train'):\n",
    "    df = dataset.FashionMnist(is_train, shuffle=True)\n",
    "    istrain = is_train == 'train'\n",
    "    \n",
    "    # ----- Image Augmentation Options -------- #\n",
    "    if istrain:\n",
    "        augs = [imgaug.Resize((64, 64))]\n",
    "    else:\n",
    "        augs = [\n",
    "            #   imgaug.CenterCrop(256, 256),\n",
    "            #   imgaug.Resize((225, 225)),\n",
    "        ]\n",
    "    df = AugmentImageComponent(df, augs)\n",
    "    # group data into batches of size 128\n",
    "    df = BatchData(df, batch_size)\n",
    "    # start 3 processes to run the dataflow in parallel\n",
    "    df = PrefetchDataZMQ(df, 5)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(GANModelDesc):\n",
    "    def __init__(self, shape, batch, z_dim):\n",
    "        self.shape = shape\n",
    "        self.batch = batch\n",
    "        self.zdim = z_dim\n",
    "        self.gen_images = None\n",
    "\n",
    "    def inputs(self):\n",
    "        return [tf.placeholder(tf.float32, [None, self.shape, self.shape], 'input')]\n",
    "\n",
    "    # genrator\n",
    "    def generator(self, z):\n",
    "        \"\"\"return an image generated from z\n",
    "        \"\"\"\n",
    "        l = FullyConnected('fc0', z, 1024 * 4 * 4, activation=tf.identity)\n",
    "        l = tf.reshape(l, [-1, 4, 4, 1024])\n",
    "        l = BNReLU(l)\n",
    "        with argscope(Conv2DTranspose, activation=BNReLU, kernel_size=4, stride=2):\n",
    "            g = (LinearWrap(l)\n",
    "                 .Conv2DTranspose('deconv_0', 512)\n",
    "                 .Conv2DTranspose('deconv_1', 256)\n",
    "                 .Conv2DTranspose('deconv_2', 128)\n",
    "                 .Conv2DTranspose('deconv_3', 1, activation=tf.identity)())    \n",
    "            g = tf.tanh(g, name='gen')\n",
    "        return g\n",
    "                   \n",
    "    # discriminator\n",
    "    @auto_reuse_variable_scope\n",
    "    def discriminator(self, imgs):\n",
    "        with argscope(Conv2D, kernel_size=4, stride=2):\n",
    "            d = (LinearWrap(imgs)\n",
    "                 .Conv2D('conv0', 64, activation=tf.nn.leaky_relu)\n",
    "                 .Conv2D('conv1', 128)\n",
    "                 .BatchNorm('bn1')\n",
    "                 .tf.nn.leaky_relu()\n",
    "                 .Conv2D('conv2', 256)\n",
    "                 .BatchNorm('bn2')\n",
    "                 .tf.nn.leaky_relu()\n",
    "                 .Conv2D('conv3', 512)\n",
    "                 .BatchNorm('bn3')\n",
    "                 .tf.nn.leaky_relu()\n",
    "                 .FullyConnected('fc1', 1,\n",
    "                                 activation=tf.identity)())                   \n",
    "        return d\n",
    "                 \n",
    "    \n",
    "    def build_graph(self, imgs):\n",
    "        # NHW to NHWC\n",
    "        imgs = tf.expand_dims(imgs, 3)\n",
    "        \n",
    "        z = tf.random_uniform([self.batch, self.zdim], -1, 1, name='z_train')\n",
    "        z = tf.placeholder_with_default(z, [None, self.zdim], name='z')         \n",
    "        \n",
    "        with argscope([Conv2D, Conv2DTranspose, FullyConnected],\n",
    "                      kernel_initializer=tf.truncated_normal_initializer(stddev=0.02)):\n",
    "            with tf.variable_scope('gen'):\n",
    "                gen_image = self.generator(z)\n",
    "            tf.summary.image('Generated-Images', gen_image, max_outputs=25)\n",
    "\n",
    "            with tf.variable_scope('discrim'):\n",
    "                logits_real = self.discriminator(imgs)\n",
    "                logits_fake = self.discriminator(gen_image)\n",
    "                 \n",
    "            self.build_losses(logits_real, logits_fake)\n",
    "            self.collect_variables()\n",
    "                 \n",
    "    \n",
    "    def optimizer(self):\n",
    "        lr = tf.get_variable('learning_rate', initializer=2e-4, trainable=False)\n",
    "        return tf.train.AdamOptimizer(lr, beta1=0.5, beta2 = 0.999, epsilon=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataset\n",
    "df = get_dataflow(100, 'train')\n",
    "steps_per_epoch = df.size()\n",
    "\n",
    "model = Model(64, 100, 100)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Configuration containing everything necessary in a training.\n",
    "\"\"\"\n",
    "config = TrainConfig(\n",
    "    model = model,\n",
    "    data = QueueInput(df),\n",
    "    callbacks = [\n",
    "        # save the model after every epoch\n",
    "        ModelSaver()\n",
    "    ],\n",
    "    steps_per_epoch = steps_per_epoch,\n",
    "    max_epoch = 10,\n",
    ")\n",
    "\n",
    "# training with CPU or GPU?\n",
    "if tf.test.gpu_device_name():c\n",
    "    logger.set_logger_dir('./GAN_log')\n",
    "    MultiGPUGANTrainer(num_gpu = 4,\n",
    "                       input = QueueInput(df),\n",
    "                       model = model).train_with_defaults(\n",
    "                                          callbacks=[PeriodicTrigger(\n",
    "                                                      ModelSaver(checkpoint_dir='./GAN_log'),\n",
    "                                                      every_k_epochs=10)],\n",
    "                                          steps_per_epoch = steps_per_epoch,\n",
    "                                          max_epoch=100)\n",
    "else:\n",
    "    logger.set_logger_dir('./GAN_log')\n",
    "    GANTrainer(input = QueueInput(df),\n",
    "               model = model).train_with_defaults(\n",
    "                                  callbacks=[PeriodicTrigger(\n",
    "                                              ModelSaver(checkpoint_dir='./GAN_log'),\n",
    "                                              every_k_epochs=10)],\n",
    "                                  steps_per_epoch = steps_per_epoch,\n",
    "                                  max_epoch=100)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the original images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[0822 06:59:22 @fs.py:100]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Env var $TENSORPACK_DATASET not set, using /root/tensorpack_data for datasets.\n",
      "\u001b[32m[0822 06:59:22 @parallel.py:291]\u001b[0m [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.\n",
      "\u001b[32m[0822 06:59:22 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0822 06:59:22 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0822 06:59:22 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0822 06:59:22 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0822 06:59:22 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n"
     ]
    }
   ],
   "source": [
    "origin = get_dataflow(25, 'train')\n",
    "origin.reset_state()\n",
    "\n",
    "for idx, dp in enumerate(origin.get_data()):\n",
    "    if idx == 0:\n",
    "        dp = dp[0] + 1\n",
    "        dp = dp * 90.0\n",
    "        img = stack_patches(dp, nr_row=5, nr_col=5)\n",
    "        cv2.imwrite('fashion_mnist.png', img)    \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[0822 06:59:24 @varmanip.py:182]\u001b[0m Checkpoint path ./GAN_log/model-60000.index is auto-corrected to ./GAN_log/model-60000.\n",
      "\u001b[32m[0822 06:59:24 @registry.py:121]\u001b[0m gen/fc0 input: [None, 100]\n",
      "\u001b[32m[0822 06:59:24 @registry.py:129]\u001b[0m gen/fc0 output: [None, 16384]\n",
      "\u001b[32m[0822 06:59:24 @registry.py:121]\u001b[0m gen/deconv_0 input: [None, 4, 4, 1024]\n",
      "\u001b[32m[0822 06:59:24 @registry.py:129]\u001b[0m gen/deconv_0 output: [None, 8, 8, 512]\n",
      "\u001b[32m[0822 06:59:24 @registry.py:121]\u001b[0m gen/deconv_1 input: [None, 8, 8, 512]\n",
      "\u001b[32m[0822 06:59:24 @registry.py:129]\u001b[0m gen/deconv_1 output: [None, 16, 16, 256]\n",
      "\u001b[32m[0822 06:59:24 @registry.py:121]\u001b[0m gen/deconv_2 input: [None, 16, 16, 256]\n",
      "\u001b[32m[0822 06:59:24 @registry.py:129]\u001b[0m gen/deconv_2 output: [None, 32, 32, 128]\n",
      "\u001b[32m[0822 06:59:24 @registry.py:121]\u001b[0m gen/deconv_3 input: [None, 32, 32, 128]\n",
      "\u001b[32m[0822 06:59:24 @registry.py:129]\u001b[0m gen/deconv_3 output: [None, 64, 64, 1]\n",
      "\u001b[32m[0822 06:59:24 @registry.py:121]\u001b[0m discrim/conv0 input: [None, 64, 64, 1]\n",
      "\u001b[32m[0822 06:59:24 @registry.py:129]\u001b[0m discrim/conv0 output: [None, 32, 32, 64]\n",
      "\u001b[32m[0822 06:59:24 @registry.py:121]\u001b[0m discrim/conv1 input: [None, 32, 32, 64]\n",
      "\u001b[32m[0822 06:59:24 @registry.py:129]\u001b[0m discrim/conv1 output: [None, 16, 16, 128]\n",
      "\u001b[32m[0822 06:59:24 @registry.py:121]\u001b[0m discrim/conv2 input: [None, 16, 16, 128]\n",
      "\u001b[32m[0822 06:59:25 @registry.py:129]\u001b[0m discrim/conv2 output: [None, 8, 8, 256]\n",
      "\u001b[32m[0822 06:59:25 @registry.py:121]\u001b[0m discrim/conv3 input: [None, 8, 8, 256]\n",
      "\u001b[32m[0822 06:59:25 @registry.py:129]\u001b[0m discrim/conv3 output: [None, 4, 4, 512]\n",
      "\u001b[32m[0822 06:59:25 @registry.py:121]\u001b[0m discrim/fc1 input: [None, 4, 4, 512]\n",
      "\u001b[32m[0822 06:59:25 @registry.py:129]\u001b[0m discrim/fc1 output: [None, 1]\n",
      "\u001b[32m[0822 06:59:25 @collection.py:164]\u001b[0m These collections were modified but restored in : (tf.GraphKeys.SUMMARIES: 0->3)\n",
      "\u001b[32m[0822 06:59:25 @sessinit.py:90]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m The following variables are in the checkpoint, but not found in the graph: global_step:0, learning_rate:0, optimize/beta1_power:0, optimize/beta2_power:0\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "\u001b[32m[0822 06:59:27 @sessinit.py:117]\u001b[0m Restoring checkpoint from ./GAN_log/model-60000 ...\n",
      "INFO:tensorflow:Restoring parameters from ./GAN_log/model-60000\n"
     ]
    }
   ],
   "source": [
    "model = Model(64, 100, 100)\n",
    "\n",
    "# loading the saved model.\n",
    "session = get_model_loader('./GAN_log/model-60000.index')\n",
    "\n",
    "pred_config = PredictConfig(\n",
    "        model = model,\n",
    "        session_init = session,\n",
    "        input_names = ['z'],\n",
    "        output_names = ['gen/gen', 'z']\n",
    "    )\n",
    "\n",
    "# make 25 images\n",
    "pred = SimpleDatasetPredictor(pred_config, RandomZData((25, 100)))\n",
    "\n",
    "for i, gen in enumerate(pred.get_result()):  \n",
    "    gen = gen[0] + 1\n",
    "    gen = gen * 90.0\n",
    "    gen = np.clip(gen, 0, 255)\n",
    "    gen = gen[:, :, :, ::-1]\n",
    "    img = stack_patches(gen, nr_row=5, nr_col=5)\n",
    "    cv2.imwrite('fashion_mnist_gen.png', img)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
