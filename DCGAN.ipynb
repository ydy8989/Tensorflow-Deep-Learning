{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN\n",
    "\n",
    "* [DCGAN](https://arxiv.org/pdf/1511.06434.pdf) with Fashion MNIST data.\n",
    "* First of all, you should import [GAN.py](https://github.com/tensorpack/tensorpack/blob/master/examples/GAN/GAN.py) at the same path.\n",
    "* [Reference](https://github.com/tensorpack/tensorpack/blob/master/examples/GAN/DCGAN.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2  \n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "from GAN import *\n",
    "from PIL import Image\n",
    "from tensorpack import *\n",
    "from tensorpack.dataflow import *\n",
    "from tensorpack.callbacks import *\n",
    "from tensorpack.tfutils import summary\n",
    "from tensorpack.utils.viz import gen_stack_patches\n",
    "from tensorpack.tfutils.scope_utils import auto_reuse_variable_scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dataflow(batch_size, is_train='train'):\n",
    "    df = dataset.FashionMnist(is_train, shuffle=True)\n",
    "    istrain = is_train == 'train'\n",
    "    \n",
    "    # ----- Image Augmentation Options -------- #\n",
    "    if istrain:\n",
    "        augs = [imgaug.Resize((64, 64))]\n",
    "    else:\n",
    "        augs = [\n",
    "            #   imgaug.CenterCrop(256, 256),\n",
    "            #   imgaug.Resize((225, 225)),\n",
    "        ]\n",
    "    df = AugmentImageComponent(df, augs)\n",
    "    # group data into batches of size 128\n",
    "    df = BatchData(df, batch_size)\n",
    "    # start 3 processes to run the dataflow in parallel\n",
    "#     df = PrefetchDataZMQ(df, 5)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(GANModelDesc):\n",
    "    def __init__(self, shape, batch, z_dim):\n",
    "        self.shape = shape\n",
    "        self.batch = batch\n",
    "        self.zdim = z_dim\n",
    "        self.gen_images = None\n",
    "\n",
    "    def inputs(self):\n",
    "        return [tf.placeholder(tf.float32, [None, self.shape, self.shape], 'input')]\n",
    "\n",
    "    # genrator\n",
    "    def generator(self, z):\n",
    "        \"\"\"return an image generated from z\n",
    "        \"\"\"\n",
    "        l = FullyConnected('fc0', z, 1024 * 4 * 4, activation=tf.identity)\n",
    "        l = tf.reshape(l, [-1, 4, 4, 1024])\n",
    "        l = BNReLU(l)\n",
    "        with argscope(Conv2DTranspose, activation=BNReLU, kernel_size=4, stride=2):\n",
    "            g = (LinearWrap(l)\n",
    "                 .Conv2DTranspose('deconv_0', 512)\n",
    "                 .Conv2DTranspose('deconv_1', 256)\n",
    "                 .Conv2DTranspose('deconv_2', 128)\n",
    "                 .Conv2DTranspose('deconv_3', 1, activation=tf.identity)())    \n",
    "            g = tf.tanh(g, name='gen')\n",
    "        return g\n",
    "                   \n",
    "    # discriminator\n",
    "    @auto_reuse_variable_scope\n",
    "    def discriminator(self, imgs):\n",
    "        with argscope(Conv2D, kernel_size=4, stride=2):\n",
    "            d = (LinearWrap(imgs)\n",
    "                 .Conv2D('conv0', 64, activation=tf.nn.leaky_relu)\n",
    "                 .Conv2D('conv1', 128)\n",
    "                 .BatchNorm('bn1')\n",
    "                 .tf.nn.leaky_relu()\n",
    "                 .Conv2D('conv2', 256)\n",
    "                 .BatchNorm('bn2')\n",
    "                 .tf.nn.leaky_relu()\n",
    "                 .Conv2D('conv3', 512)\n",
    "                 .BatchNorm('bn3')\n",
    "                 .tf.nn.leaky_relu()\n",
    "                 .FullyConnected('fc1', 1,\n",
    "                                 activation=tf.identity)())                   \n",
    "        return d\n",
    "                 \n",
    "    \n",
    "    def build_graph(self, imgs):\n",
    "        # NHW to NHWC\n",
    "        imgs = tf.expand_dims(imgs, 3)\n",
    "        \n",
    "        z = tf.random_uniform([self.batch, self.zdim], -1, 1, name='z_train')\n",
    "        z = tf.placeholder_with_default(z, [None, self.zdim], name='z')         \n",
    "        \n",
    "        with argscope([Conv2D, Conv2DTranspose, FullyConnected],\n",
    "                      kernel_initializer=tf.truncated_normal_initializer(stddev=0.02)):\n",
    "            with tf.variable_scope('gen'):\n",
    "                gen_image = self.generator(z)\n",
    "            tf.summary.image('Generated-Images', gen_image, max_outputs=25)\n",
    "\n",
    "            with tf.variable_scope('discrim'):\n",
    "                logits_real = self.discriminator(imgs)\n",
    "                logits_fake = self.discriminator(gen_image)\n",
    "                 \n",
    "            self.build_losses(logits_real, logits_fake)\n",
    "            self.collect_variables()\n",
    "                 \n",
    "    \n",
    "    def optimizer(self):\n",
    "        lr = tf.get_variable('learning_rate', initializer=2e-4, trainable=False)\n",
    "        return tf.train.AdamOptimizer(lr, beta1=0.5, beta2 = 0.999, epsilon=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset\n",
    "df = get_dataflow(100, 'train')\n",
    "steps_per_epoch = df.size()\n",
    "\n",
    "model = Model(64, 100, 100)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Configuration containing everything necessary in a training.\n",
    "\"\"\"\n",
    "config = TrainConfig(\n",
    "    model = model,\n",
    "    data = QueueInput(df),\n",
    "    callbacks = [\n",
    "        # save the model after every epoch\n",
    "        ModelSaver(checkpoint_dir='./fashion_mnist_result')\n",
    "    ],\n",
    "    steps_per_epoch = steps_per_epoch,\n",
    "    max_epoch = 10,\n",
    ")\n",
    "\n",
    "# training with CPU or GPU?\n",
    "if tf.test.gpu_device_name():\n",
    "    logger.set_logger_dir('./GAN_log')\n",
    "    MultiGPUGANTrainer(num_gpu = 4,\n",
    "                       input = QueueInput(df),\n",
    "                       model = model).train_with_defaults(\n",
    "                                          callbacks=[PeriodicTrigger(\n",
    "                                                      ModelSaver(checkpoint_dir='./GAN_log'),\n",
    "                                                      every_k_epochs=10)],\n",
    "                                          steps_per_epoch = steps_per_epoch,\n",
    "                                          max_epoch=100)\n",
    "else:\n",
    "    logger.set_logger_dir('./GAN_log')\n",
    "    GANTrainer(input = QueueInput(df),\n",
    "               model = model).train_with_defaults(\n",
    "                                  callbacks=[PeriodicTrigger(\n",
    "                                              ModelSaver(checkpoint_dir='./GAN_log'),\n",
    "                                              every_k_epochs=10)],\n",
    "                                  steps_per_epoch = steps_per_epoch,\n",
    "                                  max_epoch=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
